from datasets import load_dataset, Audio
import soundfile as sf
import io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from collections import Counter
import unicodedata

# ============================================
# 1. C·∫§U H√åNH V√Ä LOAD DATASET
# ============================================
NUM_SAMPLES = 20000
stats = []
vocab_counter = Counter()
errors = []

print(f"üöÄ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch {NUM_SAMPLES if NUM_SAMPLES else 'to√†n b·ªô'} m·∫´u...")

ds = load_dataset("nguyendv02/ViMD_Dataset", split="train", streaming=True)
ds = ds.cast_column("audio", Audio(decode=False))

# ============================================
# 2. THU TH·∫¨P D·ªÆ LI·ªÜU CHI TI·∫æT
# ============================================
for i, item in tqdm(enumerate(ds), total=NUM_SAMPLES):
    try:
        # --- X·ª≠ l√Ω Audio ---
        audio_bytes = item['audio']['bytes']
        with io.BytesIO(audio_bytes) as f:
            info = sf.info(f)
            duration = info.duration
            sample_rate = info.samplerate
            channels = info.channels
            
        # --- X·ª≠ l√Ω Text ---
        text = item.get('text', "") or ""
        
        # Ph√¢n t√≠ch k√Ω t·ª±
        vocab_counter.update(text)
        
        # T√≠nh to√°n th√™m th√¥ng tin
        words = text.split()
        num_words = len(words)
        char_per_second = len(text) / duration if duration > 0 else 0
        words_per_second = num_words / duration if duration > 0 else 0
        
        # Ph√¢n lo·∫°i tones trong ti·∫øng Vi·ªát
        tonal_marks = sum(1 for c in text if unicodedata.combining(c))
        
        stats.append({
            "id": i,
            "duration": duration,
            "sample_rate": sample_rate,
            "channels": channels,
            "text_len": len(text),
            "num_words": num_words,
            "char_per_second": char_per_second,
            "words_per_second": words_per_second,
            "avg_word_len": len(text) / num_words if num_words > 0 else 0,
            "tonal_marks": tonal_marks,
            "text": text,
            "text_lower": text.lower()
        })

        if NUM_SAMPLES and i >= NUM_SAMPLES - 1:
            break

    except Exception as e:
        errors.append({"sample": i, "error": str(e)})
        continue

df = pd.DataFrame(stats)

# ============================================
# 3. PH√ÇN T√çCH TO√ÄN DI·ªÜN
# ============================================
print("\n" + "="*60)
print("üìä B√ÅO C√ÅO PH√ÇN T√çCH TO√ÄN DI·ªÜN D·ªÆ LI·ªÜU ASR TI·∫æNG VI·ªÜT")
print("="*60)

# --- 1. T·ªîNG QUAN CHUNG ---
print(f"\n1Ô∏è‚É£ T·ªîNG QUAN CHUNG:")
print(f"   ‚Ä¢ T·ªïng m·∫´u ƒë√£ ph√¢n t√≠ch: {len(df)}")
print(f"   ‚Ä¢ T·ª∑ l·ªá th√†nh c√¥ng: {len(df) / NUM_SAMPLES * 100:.2f}%")
print(f"   ‚Ä¢ S·ªë l·ªói: {len(errors)}")
total_duration_hours = df['duration'].sum() / 3600
print(f"   ‚Ä¢ T·ªïng th·ªùi l∆∞·ª£ng: {total_duration_hours:.2f} gi·ªù ({df['duration'].sum() / 60:.1f} ph√∫t)")
print(f"   ‚Ä¢ Th·ªùi l∆∞·ª£ng b√¨nh qu√¢n: {df['duration'].mean():.2f} ¬± {df['duration'].std():.2f} gi√¢y")

# --- 2. PH√ÇN T√çCH AUDIO DURATION ---
print(f"\n2Ô∏è‚É£ PH√ÇN T√çCH AUDIO DURATION:")
print(f"   Min: {df['duration'].min():.3f}s | Max: {df['duration'].max():.2f}s")
print(f"   Median: {df['duration'].median():.2f}s | Q1: {df['duration'].quantile(0.25):.2f}s | Q3: {df['duration'].quantile(0.75):.2f}s")

# Ph√¢n lo·∫°i kho·∫£ng th·ªùi l∆∞·ª£ng
ranges = [(0, 1), (1, 5), (5, 10), (10, 20), (20, 30), (30, 60), (60, float('inf'))]
print(f"\n   üìà Ph√¢n b·ªë theo kho·∫£ng th·ªùi l∆∞·ª£ng:")
for start, end in ranges:
    count = len(df[(df['duration'] >= start) & (df['duration'] < end)])
    pct = count / len(df) * 100
    label = f"{start}-{end}s" if end != float('inf') else f">{start}s"
    print(f"      {label}: {count:5d} ({pct:5.1f}%)")

# C·∫£nh b√°o
n_short = len(df[df['duration'] < 0.5])
n_long = len(df[df['duration'] > 60])
print(f"\n   ‚ö†Ô∏è C·∫¢NH B√ÅO: {n_short} m·∫´u < 0.5s (qu√° ng·∫Øn), {n_long} m·∫´u > 60s (qu√° d√†i)")

# --- 3. PH√ÇN T√çCH SAMPLE RATE ---
print(f"\n3Ô∏è‚É£ PH√ÇN T√çCH SAMPLE RATE & AUDIO FORMAT:")
sr_dist = df['sample_rate'].value_counts()
for sr, count in sr_dist.items():
    print(f"   ‚Ä¢ {sr} Hz: {count} m·∫´u ({count/len(df)*100:.2f}%)")
print(f"   ‚Ä¢ Channels: {df['channels'].unique()}")

# --- 4. PH√ÇN T√çCH TEXT & T·ªêC ƒê·ªò PH√ÅT √ÇM ---
print(f"\n4Ô∏è‚É£ PH√ÇN T√çCH TEXT & T·ªêC ƒê·ªò PH√ÅT √ÇM:")
print(f"   ‚Ä¢ T·ªïng k√Ω t·ª±: {df['text_len'].sum()}")
print(f"   ‚Ä¢ Chi·ªÅu d√†i text - Min: {df['text_len'].min()} | Max: {df['text_len'].max()} | Trung b√¨nh: {df['text_len'].mean():.1f}")
print(f"   ‚Ä¢ T·ªïng t·ª´: {df['num_words'].sum()}")
print(f"   ‚Ä¢ T·ª´ b√¨nh qu√¢n/m·∫´u: {df['num_words'].mean():.1f} ¬± {df['num_words'].std():.2f}")
print(f"   ‚Ä¢ ƒê·ªô d√†i t·ª´ b√¨nh qu√¢n: {df['avg_word_len'].mean():.2f} k√Ω t·ª±")

print(f"\n   üìä T·ªëc ƒë·ªô ph√°t √¢m:")
print(f"      ‚Ä¢ K√Ω t·ª±/gi√¢y: {df['char_per_second'].mean():.2f} ¬± {df['char_per_second'].std():.2f}")
print(f"      ‚Ä¢ T·ª´/gi√¢y: {df['words_per_second'].mean():.2f} ¬± {df['words_per_second'].std():.2f}")

# --- 5. PH√ÇN T√çCH VOCABULARY ---
print(f"\n5Ô∏è‚É£ PH√ÇN T√çCH VOCABULARY:")
unique_chars = len(vocab_counter)
print(f"   ‚Ä¢ T·ªïng k√Ω t·ª± unique: {unique_chars}")
print(f"   ‚Ä¢ T·ªïng k√Ω t·ª± (v·ªõi l·∫∑p): {sum(vocab_counter.values())}")
chars_sorted = sorted(vocab_counter.keys())
print(f"   ‚Ä¢ Danh s√°ch k√Ω t·ª±: {''.join(chars_sorted[:100])}")

print(f"\n   üîù Top 20 k√Ω t·ª± xu·∫•t hi·ªán nhi·ªÅu nh·∫•t:")
for idx, (char, count) in enumerate(vocab_counter.most_common(20), 1):
    char_repr = repr(char) if char == ' ' else char
    print(f"      {idx:2d}. {char_repr}: {count:6d} ({count/sum(vocab_counter.values())*100:5.2f}%)")

# --- 6. PH√ÇN T√çCH TONAL MARKS (D·∫•u tone ti·∫øng Vi·ªát) ---
print(f"\n6Ô∏è‚É£ PH√ÇN T√çCH D·∫§U TONE TI·∫æNG VI·ªÜT:")
total_tonal = df['tonal_marks'].sum()
print(f"   ‚Ä¢ T·ªïng d·∫•u tone: {total_tonal}")
print(f"   ‚Ä¢ B√¨nh qu√¢n d·∫•u tone/m·∫´u: {df['tonal_marks'].mean():.2f}")
print(f"   ‚Ä¢ M·∫´u kh√¥ng c√≥ d·∫•u tone: {len(df[df['tonal_marks'] == 0])}")

# --- 7. PH√ÇN T√çCH CORRELATION ---
print(f"\n7Ô∏è‚É£ PH√ÇN T√çCH T∆Ø∆†NG QUAN (CORRELATION):")
corr_matrix = df[['duration', 'text_len', 'num_words', 'char_per_second']].corr()
print(f"   ‚Ä¢ Correlation Duration ‚Üî Text Length: {corr_matrix.loc['duration', 'text_len']:.3f}")
print(f"   ‚Ä¢ Correlation Duration ‚Üî Num Words: {corr_matrix.loc['duration', 'num_words']:.3f}")
print(f"   ‚Ä¢ Correlation Text Length ‚Üî Num Words: {corr_matrix.loc['text_len', 'num_words']:.3f}")

# --- 8. PH√ÇN T√çCH OUTLIERS ---
print(f"\n8Ô∏è‚É£ PH√ÇN T√çCH OUTLIERS:")
Q1_dur = df['duration'].quantile(0.25)
Q3_dur = df['duration'].quantile(0.75)
IQR_dur = Q3_dur - Q1_dur
outliers_dur = df[(df['duration'] < Q1_dur - 1.5*IQR_dur) | (df['duration'] > Q3_dur + 1.5*IQR_dur)]
print(f"   ‚Ä¢ Audio Duration Outliers: {len(outliers_dur)} ({len(outliers_dur)/len(df)*100:.2f}%)")

Q1_text = df['text_len'].quantile(0.25)
Q3_text = df['text_len'].quantile(0.75)
IQR_text = Q3_text - Q1_text
outliers_text = df[(df['text_len'] < Q1_text - 1.5*IQR_text) | (df['text_len'] > Q3_text + 1.5*IQR_text)]
print(f"   ‚Ä¢ Text Length Outliers: {len(outliers_text)} ({len(outliers_text)/len(df)*100:.2f}%)")

# --- 9. PH√ÇN T√çCH MISSING/EMPTY VALUES ---
print(f"\n9Ô∏è‚É£ PH√ÇN T√çCH D·ªÆ LI·ªÜU THI·∫æU/R·ªñNG:")
empty_text = len(df[df['text_len'] == 0])
print(f"   ‚Ä¢ M·∫´u text r·ªóng: {empty_text} ({empty_text/len(df)*100:.2f}%)")
zero_duration = len(df[df['duration'] == 0])
print(f"   ‚Ä¢ M·∫´u duration = 0: {zero_duration} ({zero_duration/len(df)*100:.2f}%)")

# --- 10. G·ª¢I √ù TI·ªÄN X·ª¨ L√ù ---
print(f"\nüîü G·ª¢I √ù TI·ªÄN X·ª¨ L√ù CHO M√î H√åNH ASR:")
print(f"   ‚úì L·ªçc b·ªè: Duration < 0.5s ho·∫∑c > 60s ({len(df[(df['duration'] < 0.5) | (df['duration'] > 60)])}) m·∫´u")
print(f"   ‚úì L·ªçc b·ªè: Text r·ªóng ({empty_text} m·∫´u)")
print(f"   ‚úì Chu·∫©n h√≥a: Sample rate th√†nh 16kHz ho·∫∑c 8kHz")
print(f"   ‚úì D·ª± ki·∫øn sau l·ªçc: {len(df[(df['duration'] >= 0.5) & (df['duration'] <= 60) & (df['text_len'] > 0)])} m·∫´u")

# ============================================
# 4. VISUALIZATION
# ============================================
sns.set_style("whitegrid")
fig = plt.figure(figsize=(20, 14))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# Bi·ªÉu ƒë·ªì 1: Ph√¢n ph·ªëi Duration
ax1 = fig.add_subplot(gs[0, 0])
sns.histplot(data=df, x='duration', kde=True, bins=50, ax=ax1, color='skyblue')
ax1.set_title('Ph√¢n ph·ªëi ƒê·ªô d√†i Audio', fontsize=12, fontweight='bold')
ax1.axvline(df['duration'].mean(), color='r', linestyle='--', label=f"Mean: {df['duration'].mean():.2f}s")
ax1.legend()

# Bi·ªÉu ƒë·ªì 2: Ph√¢n ph·ªëi Text Length
ax2 = fig.add_subplot(gs[0, 1])
sns.histplot(data=df, x='text_len', kde=True, bins=50, ax=ax2, color='orange')
ax2.set_title('Ph√¢n ph·ªëi ƒê·ªô d√†i Text', fontsize=12, fontweight='bold')

# Bi·ªÉu ƒë·ªì 3: Ph√¢n ph·ªëi Num Words
ax3 = fig.add_subplot(gs[0, 2])
sns.histplot(data=df, x='num_words', kde=True, bins=50, ax=ax3, color='lightgreen')
ax3.set_title('Ph√¢n ph·ªëi S·ªë T·ª´', fontsize=12, fontweight='bold')

# Bi·ªÉu ƒë·ªì 4: Scatter Duration vs Text Length
ax4 = fig.add_subplot(gs[1, 0])
sns.scatterplot(data=df, x='duration', y='text_len', alpha=0.4, ax=ax4, color='purple')
ax4.set_title('T∆∞∆°ng quan: Duration vs Text Length', fontsize=12, fontweight='bold')
ax4.set_ylabel('Text Length')

# Bi·ªÉu ƒë·ªì 5: Scatter Duration vs Char per Second
ax5 = fig.add_subplot(gs[1, 1])
sns.scatterplot(data=df, x='duration', y='char_per_second', alpha=0.4, ax=ax5, color='teal')
ax5.set_title('T∆∞∆°ng quan: Duration vs Char/Second', fontsize=12, fontweight='bold')
ax5.set_ylabel('Char per Second')

# Bi·ªÉu ƒë·ªì 6: Boxplot Duration (Outliers)
ax6 = fig.add_subplot(gs[1, 2])
sns.boxplot(y=df['duration'], ax=ax6, color='lightcoral')
ax6.set_title('Boxplot Duration (Ph√°t hi·ªán Outlier)', fontsize=12, fontweight='bold')

# Bi·ªÉu ƒë·ªì 7: Top 15 k√Ω t·ª±
ax7 = fig.add_subplot(gs[2, :2])
top_chars = vocab_counter.most_common(15)
chars_labels = [c[0] if c[0] != ' ' else '[SPACE]' for c in top_chars]
chars_values = [c[1] for c in top_chars]
ax7.barh(chars_labels, chars_values, color='steelblue')
ax7.set_title('Top 15 K√Ω t·ª± Xu·∫•t hi·ªán Nhi·ªÅu nh·∫•t', fontsize=12, fontweight='bold')
ax7.invert_yaxis()

# Bi·ªÉu ƒë·ªì 8: Ph√¢n b·ªë Duration theo kho·∫£ng
ax8 = fig.add_subplot(gs[2, 2])
duration_bins = [(0, 1), (1, 5), (5, 10), (10, 20), (20, 30), (30, 60), (60, 1000)]
bin_labels = ['<1s', '1-5s', '5-10s', '10-20s', '20-30s', '30-60s', '>60s']
bin_counts = [len(df[(df['duration'] >= s) & (df['duration'] < e)]) for s, e in duration_bins]
ax8.bar(bin_labels, bin_counts, color='coral')
ax8.set_title('Ph√¢n b·ªë Duration theo Kho·∫£ng', fontsize=12, fontweight='bold')
ax8.tick_params(axis='x', rotation=45)

plt.suptitle('üìä Ph√¢n T√≠ch Chi Ti·∫øt Dataset ASR Ti·∫øng Vi·ªát', fontsize=16, fontweight='bold', y=0.995)
plt.show()