import json
import statistics
from collections import Counter
from pathlib import Path

def analyze_jsonl(file_path):
    """Thá»‘ng kÃª dá»¯ liá»‡u tá»« file JSONL"""
    
    entries = []
    
    # Äá»c file JSONL
    print(f"ğŸ“– Äang Ä‘á»c file: {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                entry = json.loads(line.strip())
                entries.append(entry)
            except json.JSONDecodeError as e:
                print(f"âš ï¸  Lá»—i á»Ÿ dÃ²ng {line_num}: {e}")
    
    if not entries:
        print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch!")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š THá»NG KÃŠ Tá»”NG QUÃT")
    print(f"{'='*60}")
    print(f"Tá»•ng sá»‘ entries: {len(entries)}")
    
    # Thá»‘ng kÃª tá»«ng trÆ°á»ng
    files = [e.get('file') for e in entries]
    texts = [e.get('text') for e in entries]
    durations = [e.get('duration') for e in entries]
    sample_rates = [e.get('original_sample_rate') for e in entries]
    
    # File statistics
    print(f"\nğŸ“ FILE:")
    print(f"  - Sá»‘ entries cÃ³ 'file': {sum(1 for f in files if f)}")
    print(f"  - Sá»‘ entries khÃ´ng cÃ³ 'file': {sum(1 for f in files if not f)}")
    if any(files):
        unique_files = len(set(f for f in files if f))
        print(f"  - Sá»‘ file duy nháº¥t: {unique_files}")
    
    # Text statistics
    print(f"\nğŸ“ TEXT:")
    texts_valid = [t for t in texts if t]
    print(f"  - Sá»‘ entries cÃ³ 'text': {len(texts_valid)}")
    print(f"  - Sá»‘ entries khÃ´ng cÃ³ 'text': {sum(1 for t in texts if not t)}")
    
    if texts_valid:
        text_lengths = [len(str(t)) for t in texts_valid]
        word_counts = [len(str(t).split()) for t in texts_valid]
        
        print(f"  - Äá»™ dÃ i text (kÃ½ tá»±):")
        print(f"    â€¢ Min: {min(text_lengths)}")
        print(f"    â€¢ Max: {max(text_lengths)}")
        print(f"    â€¢ Avg: {statistics.mean(text_lengths):.2f}")
        print(f"    â€¢ Median: {statistics.median(text_lengths):.2f}")
        
        print(f"  - Sá»‘ tá»« trong text:")
        print(f"    â€¢ Min: {min(word_counts)}")
        print(f"    â€¢ Max: {max(word_counts)}")
        print(f"    â€¢ Avg: {statistics.mean(word_counts):.2f}")
        print(f"    â€¢ Median: {statistics.median(word_counts):.2f}")
    
    # Duration statistics
    print(f"\nâ±ï¸  DURATION:")
    durations_valid = [d for d in durations if d is not None and isinstance(d, (int, float))]
    print(f"  - Sá»‘ entries cÃ³ 'duration': {len(durations_valid)}")
    print(f"  - Sá»‘ entries khÃ´ng cÃ³ 'duration': {sum(1 for d in durations if d is None)}")
    
    if durations_valid:
        print(f"  - GiÃ¡ trá»‹ duration (giÃ¢y):")
        print(f"    â€¢ Min: {min(durations_valid):.2f}")
        print(f"    â€¢ Max: {max(durations_valid):.2f}")
        print(f"    â€¢ Avg: {statistics.mean(durations_valid):.2f}")
        print(f"    â€¢ Median: {statistics.median(durations_valid):.2f}")
        print(f"    â€¢ Total: {sum(durations_valid):.2f} giÃ¢y ({sum(durations_valid)/60:.2f} phÃºt)")
    
    # Sample rate statistics
    print(f"\nğŸµ ORIGINAL_SAMPLE_RATE:")
    sample_rates_valid = [s for s in sample_rates if s is not None]
    print(f"  - Sá»‘ entries cÃ³ 'original_sample_rate': {len(sample_rates_valid)}")
    print(f"  - Sá»‘ entries khÃ´ng cÃ³ 'original_sample_rate': {sum(1 for s in sample_rates if s is None)}")
    
    if sample_rates_valid:
        rate_counter = Counter(sample_rates_valid)
        print(f"  - Sample rates khÃ¡c nhau:")
        for rate, count in sorted(rate_counter.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(entries)) * 100
            print(f"    â€¢ {rate} Hz: {count} entries ({percentage:.1f}%)")
    
    print(f"\n{'='*60}")
    
    # Máº«u dá»¯ liá»‡u
    print(f"\nğŸ“‹ MáºªU Dá»® LIá»†U (3 entries Ä‘áº§u tiÃªn):")
    print(f"{'='*60}")
    for i, entry in enumerate(entries[:3], 1):
        print(f"\nEntry {i}:")
        print(json.dumps(entry, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    # Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n file cá»§a báº¡n á»Ÿ Ä‘Ã¢y
    file_path = r"D:\chuyen_nganh\ASRProject\merged_dataset_all_filtered.jsonl"
    
    if not Path(file_path).exists():
        print(f"âŒ File khÃ´ng tá»“n táº¡i: {file_path}")
    else:
        analyze_jsonl(file_path)