import torch
import torch.multiprocessing as mp
from transformers import AutoModelForCausalLM, AutoTokenizer
import os
import math
from tqdm import tqdm

# --- C·∫§U H√åNH ---
MODEL_ID = "Qwen/Qwen2.5-7B-Instruct"
INPUT_FILE = "input.txt"
OUTPUT_FILE = "output_merged.txt"
BATCH_SIZE = 8       # Batch size cho M·ªñI GPU (T·ªïng throughput = 16)
NUM_GPUS = 2         # B·∫°n c√≥ 2 GPU T4

def load_model_on_gpu(gpu_id):
    """
    H√†m load model ri√™ng cho t·ª´ng GPU
    """
    device = f"cuda:{gpu_id}"
    print(f"[GPU {gpu_id}] ƒêang t·∫£i model...")
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Load model c·ª• th·ªÉ v√†o GPU ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map=device, # √âp model n·∫±m tr·ªçn tr√™n GPU n√†y
        torch_dtype="auto",
        # load_in_4bit=True,
        # bnb_4bit_compute_dtype=torch.float16
    )
    return model, tokenizer

def run_worker(gpu_id, lines, output_temp_file):
    """
    Worker x·ª≠ l√Ω m·ªôt ph·∫ßn d·ªØ li·ªáu tr√™n GPU ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    """
    try:
        model, tokenizer = load_model_on_gpu(gpu_id)
        
        system_prompt = "You are a professional translator. Translate the following Vietnamese text to English. Do not add any explanation."
        
        translated_lines = []
        
        # Batch processing
        total_batches = math.ceil(len(lines) / BATCH_SIZE)
        
        # S·ª≠ d·ª•ng tqdm v·ªõi v·ªã tr√≠ kh√°c nhau ƒë·ªÉ kh√¥ng ƒë√® l√™n nhau tr√™n terminal
        progress_bar = tqdm(total=len(lines), desc=f"GPU {gpu_id}", position=gpu_id, leave=True)

        for i in range(0, len(lines), BATCH_SIZE):
            batch_texts = lines[i : i + BATCH_SIZE]
            
            # Chu·∫©n b·ªã Prompt
            batch_prompts = []
            for text in batch_texts:
                messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": text}]
                prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                batch_prompts.append(prompt)

            # Tokenize & Move to GPU
            inputs = tokenizer(
                batch_prompts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=2048
            ).to(f"cuda:{gpu_id}")

            # Generate
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.3,
                    top_p=0.9,
                    pad_token_id=tokenizer.pad_token_id
                )

            # Decode
            input_len = inputs.input_ids.shape[1]
            output_ids = generated_ids[:, input_len:]
            decoded = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
            
            # L∆∞u k·∫øt qu·∫£ t·∫°m v√†o list
            cleaned = [s.strip().replace("\n", " ") for s in decoded]
            translated_lines.extend(cleaned)
            
            progress_bar.update(len(batch_texts))
        
        progress_bar.close()

        # Ghi file t·∫°m c·ªßa GPU n√†y
        print(f"[GPU {gpu_id}] Ho√†n th√†nh! ƒêang ghi file t·∫°m: {output_temp_file}")
        with open(output_temp_file, "w", encoding="utf-8") as f:
            for line in translated_lines:
                f.write(line + "\n")
                
    except Exception as e:
        print(f"[GPU {gpu_id}] L·ªói nghi√™m tr·ªçng: {e}")

def main():
    # 1. C·∫•u h√¨nh Multiprocessing cho CUDA
    # B·∫Øt bu·ªôc d√πng 'spawn' khi l√†m vi·ªác v·ªõi CUDA multiprocess
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass

    # 2. ƒê·ªçc d·ªØ li·ªáu
    if not os.path.exists(INPUT_FILE):
        print("Kh√¥ng t√¨m th·∫•y file input.")
        # T·∫°o file gi·∫£ l·∫≠p ƒë·ªÉ test
        with open(INPUT_FILE, "w", encoding="utf-8") as f:
            f.write("Xin ch√†o\n" * 20) 
        print("ƒê√£ t·∫°o file m·∫´u input.txt")
        
    print(f"üìÇ ƒêang ƒë·ªçc {INPUT_FILE}...")
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        all_lines = [line.strip() for line in f if line.strip()]
    
    total_lines = len(all_lines)
    print(f"üìä T·ªïng s·ªë d√≤ng: {total_lines}")

    # 3. Chia d·ªØ li·ªáu cho c√°c GPU (Splitting)
    # GPU 0 l·∫•y n·ª≠a ƒë·∫ßu, GPU 1 l·∫•y n·ª≠a sau
    chunk_size = math.ceil(total_lines / NUM_GPUS)
    chunks = [all_lines[i:i + chunk_size] for i in range(0, total_lines, chunk_size)]
    
    # ƒê·∫£m b·∫£o list chunks ƒë·ªß ƒë·ªô d√†i b·∫±ng s·ªë GPU (tr∆∞·ªùng h·ª£p file √≠t d√≤ng)
    while len(chunks) < NUM_GPUS:
        chunks.append([])

    # 4. Kh·ªüi t·∫°o Process
    processes = []
    temp_files = []

    print(f"üöÄ B·∫Øt ƒë·∫ßu kh·ªüi ch·∫°y {NUM_GPUS} workers tr√™n 2 GPU T4...")
    
    for rank in range(NUM_GPUS):
        temp_file = f"temp_output_gpu_{rank}.txt"
        temp_files.append(temp_file)
        
        # T·∫°o process
        p = mp.Process(
            target=run_worker, 
            args=(rank, chunks[rank], temp_file)
        )
        p.start()
        processes.append(p)

    # 5. ƒê·ª£i t·∫•t c·∫£ ho√†n th√†nh (Join)
    for p in processes:
        p.join()

    print("\n‚úÖ T·∫•t c·∫£ GPU ƒë√£ x·ª≠ l√Ω xong. ƒêang g·ªôp file...")

    # 6. G·ªôp k·∫øt qu·∫£ (Merging)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as outfile:
        for temp_file in temp_files:
            if os.path.exists(temp_file):
                with open(temp_file, "r", encoding="utf-8") as infile:
                    outfile.write(infile.read())
                # X√≥a file t·∫°m
                os.remove(temp_file)
    
    print(f"üéâ Ho√†n t·∫•t! K·∫øt qu·∫£ l∆∞u t·∫°i: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()